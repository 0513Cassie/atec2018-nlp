{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\servers\\python36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import re\n",
    "import gensim\n",
    "import time\n",
    "import multiprocessing\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "new_words = \"花呗 借呗 蚂蚁花呗 蚂蚁借呗 蚂蚁森林 小黄车 飞猪 微客 宝卡 芝麻信用 亲密付 淘票票 饿了么 摩拜 滴滴 滴滴出行\".split(\" \")\n",
    "for word in new_words:\n",
    "    jieba.add_word(word)\n",
    "\n",
    "star = re.compile(\"\\*+\")\n",
    "\n",
    "# df = pd.read_csv(\"data/atec_nlp_sim_train.csv\",encoding=\"utf8\",sep=\"\\t\",header=None)\n",
    "# df.set_index(0,inplace=True)\n",
    "# for i in range(1, len(df)+1):\n",
    "#     df.loc[i,4] = synonyms.compare(df.loc[i,1], df.loc[i,2])\n",
    "\n",
    "lines1 = open(\"data/atec_nlp_sim_train.csv\",\"r\",encoding=\"utf8\").readlines()\n",
    "lines2 = open(\"data/atec_nlp_sim_train_add.csv\",\"r\",encoding=\"utf8\").readlines()\n",
    "\n",
    "with open(\"data/atec_nlp_sim_test.csv\",\"w\",encoding=\"utf8\") as f:\n",
    "    for line in lines1:\n",
    "        lineno, s1, s2, label=line.strip().split(\"\\t\")\n",
    "        f.write(\"\\t\".join([lineno,s1,s2])+\"\\n\")\n",
    "        \n",
    "with open(\"data/atec_nlp_sim_test_add.csv\",\"w\",encoding=\"utf8\") as f:\n",
    "    for line in lines2:\n",
    "        lineno, s1, s2, label=line.strip().split(\"\\t\")\n",
    "        f.write(\"\\t\".join([lineno,s1,s2])+\"\\n\")\n",
    "        \n",
    "\n",
    "with open(\"data/atec_nlp_sim_train_pos.csv\",\"w\",encoding=\"utf8\") as f1, open(\"data/atec_nlp_sim_train_neg.csv\",\"w\",encoding=\"utf8\") as f2:\n",
    "    for line in lines1:\n",
    "        lineno, s1, s2, label=line.strip().split(\"\\t\")\n",
    "        if label == \"1\":\n",
    "            f1.write(\"\\t\".join([lineno,s1,s2])+\"\\n\")\n",
    "        else:\n",
    "            f2.write(\"\\t\".join([lineno,s1,s2])+\"\\n\")\n",
    "\n",
    "vocab = {}\n",
    "\n",
    "for line in lines1+lines2:\n",
    "    lineno, s1, s2, label=line.strip().split(\"\\t\")\n",
    "    \n",
    "    for word in list(jieba.cut(star.sub(\"1\",s1))) + list(jieba.cut(star.sub(\"1\",s2))):\n",
    "        vocab[word] = vocab.setdefault(word,0) + 1\n",
    "\n",
    "vocablist = [(k,v) for k,v in vocab.items()]\n",
    "vocablist.sort(key=lambda x:x[1], reverse=True)\n",
    "\n",
    "totalword = sum([c[1] for c in vocablist ])\n",
    "print(totalword)\n",
    "word_freq = {k:v/totalword for k,v in vocablist}\n",
    "\n",
    "open(\"model/word_freq.txt\",\"w\",encoding=\"utf8\").write(\"\\n\".join([k+\"\\t\"+str(v) for k,v in word_freq.items()]))\n",
    "\n",
    "word_freq = {kv.split(\"\\t\")[0]:(kv.split(\"\\t\")[1]) for kv in open(\"model/word_freq.txt\",\"r\",encoding=\"utf8\").read().split(\"\\n\")}\n",
    "\n",
    "print(len(vocablist))\n",
    "print(vocablist[:100])\n",
    "\n",
    "lineno, s1, s2, label = lines2[0].strip().split(\"\\t\")\n",
    "list(jieba.cut(s1))\n",
    "\n",
    "print(s2)\n",
    "list(jieba.cut(s2))\n",
    "\n",
    "\n",
    "\n",
    "vocab = {}\n",
    "for line in sentences:\n",
    "    for word in line:\n",
    "        vocab[word] = vocab.setdefault(word,0) + 1\n",
    "len(vocab)\n",
    "\n",
    "model = gensim.models.Word2Vec.load(\"model/word2vec_gensim\")\n",
    "\n",
    "model.wv.similar_by_word(\"淘宝\")\n",
    "\n",
    "len(model.wv.vocab)\n",
    "\n",
    "model.wv.vocab[\"微信\"].index\n",
    "model.wv.vocab[\"淘宝\"].index\n",
    "\n",
    "model.wv.vocab[\"微信\"].sample_int\n",
    "model.wv.vocab[\"淘宝\"].sample_int\n",
    "\n",
    "model.wv.vocab[\"微信\"].count\n",
    "model.wv.vocab[\"淘宝\"].count\n",
    "# (\"微信\")\n",
    "# model.wv.word_vec[\"微信\"]\n",
    "\n",
    "total = sum([v for k,v in vocab.items()])\n",
    "\n",
    "def sentence_to_vec(sentence_list, embedding_size=100, total=1e8, a=1e-3):\n",
    "    total = model.wv.vocab[\"淘宝\"].sample_int\n",
    "    sentence_set = []\n",
    "    for sentence in sentence_list:\n",
    "        vs = np.zeros(embedding_size)  # add all word2vec values into one vector for the sentence\n",
    "        sentence_length = len(sentence)\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                a_value = a / (a + model.wv.vocab[word].count/total)  # smooth inverse frequency, SIF\n",
    "                vs = np.add(vs, np.multiply(a_value, model.wv[word]))  # vs += sif * word_vector\n",
    "            except:\n",
    "                a_value = 1\n",
    "                vs = np.add(vs, np.zeros(embedding_size))\n",
    "\n",
    "        vs = np.divide(vs, sentence_length)  # weighted average\n",
    "        sentence_set.append(vs)  # add to our existing re-calculated set of sentences\n",
    "\n",
    "    # calculate PCA of this sentence set\n",
    "    pca = PCA(n_components=embedding_size)\n",
    "    pca.fit(np.array(sentence_set))\n",
    "    u = pca.components_[0]  # the PCA vector\n",
    "    u = np.multiply(u, np.transpose(u))  # u x uT\n",
    "\n",
    "    # pad the vector?  (occurs if we have less sentences than embeddings_size)\n",
    "    if len(u) < embedding_size:\n",
    "        for i in range(embedding_size - len(u)):\n",
    "            u = np.append(u, 0)  # add needed extension for multiplication below\n",
    "\n",
    "    # resulting sentence vectors, vs = vs -u x uT x vs\n",
    "    sentence_vecs = []\n",
    "    for vs in sentence_set:\n",
    "        sub = np.multiply(u, vs)\n",
    "        sentence_vecs.append(np.subtract(vs, sub))\n",
    "\n",
    "    return sentence_vecs\n",
    "\n",
    "# total_a = [(np.power(10.,total), 1/np.power(10.,a)) for a in range(3,10,3) for total in range(5,15,4)]\n",
    "# for total,a in total_a:\n",
    "#     print(\"condition:\",total,a)\n",
    "\n",
    "train_lines,test_lines = train_test_split(lines1 + lines2,test_size=0.3,random_state=13 )\n",
    "\n",
    "e = np.zeros((len(test_lines),2))\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "end = 0\n",
    "batch_size = 10000\n",
    "for i,line in tqdm_notebook(enumerate(test_lines)):\n",
    "    lineno, s1, s2, label=line.strip().split(\"\\t\")    \n",
    "    sentences.append(list(jieba.cut(star.sub(\"1\",s1))) )\n",
    "    sentences.append(list(jieba.cut(star.sub(\"1\",s2))) )\n",
    "    labels.append(label)\n",
    "    if (i+1) % batch_size == 0:\n",
    "        sentence_vecs = sentence_to_vec(sentences)\n",
    "        for j in range(len(sentences)//2):\n",
    "            sim = cosine_similarity([sentence_vecs[j*2]], [sentence_vecs[j*2 + 1]])\n",
    "            e[end+j] = (sim[0][0],labels[j])\n",
    "        end = i+1\n",
    "        sentences = []\n",
    "        labels = []\n",
    "\n",
    "if sentences:\n",
    "    sentence_vecs = sentence_to_vec(sentences)\n",
    "    for j in range(len(sentences)//2):\n",
    "        sim = cosine_similarity([sentence_vecs[j*2]], [sentence_vecs[j*2 + 1]])\n",
    "        e[end+j] = (sim[0][0],labels[j])\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "# 评估相关性\n",
    "f = pd.DataFrame(e)\n",
    "m1,m2,fact = 0,99,100\n",
    "\n",
    "print(f.corr())\n",
    "x = np.array([f1_score(y_pred=f.loc[:,0]>thr/fact, y_true=f.loc[:,1]) for thr in range(m1,m2)])\n",
    "print(list(range(m1,m2))[x.argmax()]/fact)\n",
    "\n",
    "plt.plot(x)\n",
    "# word2vec_v1   0.39712386\n",
    "# word2vec_v2   0.35729799\n",
    "\n",
    "print(chr(0x4E00))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 拼音模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinyin\n",
    "print(pinyin.get('你 好'))\n",
    "print(pinyin.get('你好', format=\"strip\", delimiter=\" \"))\n",
    "print (pinyin.get('你好', format=\"numerical\", delimiter=\" \"))\n",
    "print (pinyin.get_initial('你好'))\n",
    "pinyin.pinyin.pinyin_dict, \n",
    "pinyin.pinyin.pinyin_tone,\n",
    "len(set(pinyin.pinyin.pinyin_dict.values())),\n",
    "set(pinyin.pinyin.pinyin_tone.values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simaese model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import Input, Embedding, LSTM, Merge,Dropout,Bidirectional,CuDNNLSTM,CuDNNGRU\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adadelta,SGD,Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "import gensim\n",
    "import jieba\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from simaese import get_model,f1\n",
    "\n",
    "# model = gensim.models.Word2Vec.load(\"submits/word2vec_1.0/model/word2vec_gensim\")\n",
    "model = gensim.models.Word2Vec.load(\"model/word2vec_gensim256\")\n",
    "word2index = {v:k for k,v in enumerate(model.wv.index2word)}\n",
    "\n",
    "open(\"model/word2index.txt\",\"w\",encoding=\"utf8\").write(\"\\n\".join([k+\"\\t\"+str(v) for k,v in word2index.items()]))\n",
    "\n",
    "word2index = {kv.split(\"\\t\")[0]:(kv.split(\"\\t\")[1]) for kv in open(\"model/word2index.txt\",\"r\",encoding=\"utf8\").read().split(\"\\n\")}\n",
    "\n",
    "\n",
    "\n",
    "lines1 = open(\"data/atec_nlp_sim_train.csv\",\"r\",encoding=\"utf8\").readlines()\n",
    "lines2 = open(\"data/atec_nlp_sim_train_add.csv\",\"r\",encoding=\"utf8\").readlines()\n",
    "\n",
    "data_l_n = []\n",
    "data_r_n = []\n",
    "y = []\n",
    "for line in lines1+lines2:\n",
    "    lineno, s1, s2, label=line.strip().split(\"\\t\")\n",
    "    data_l_n.append([word2index[word] for word in list(jieba.cut(star.sub(\"1\",s1))) if word in word2index]) \n",
    "    data_r_n.append([word2index[word] for word in list(jieba.cut(star.sub(\"1\",s2))) if word in word2index])\n",
    "    y.append(int(label))\n",
    "# 对齐语料中句子的长度 \n",
    "max_length = 20\n",
    "data_l_n = pad_sequences(train_l, maxlen=max_length)\n",
    "data_r_n = pad_sequences(train_r, maxlen=max_length)\n",
    "\n",
    "train_data_l_n,test_data_l_n,train_data_r_n,test_data_r_n,train_y,test_y = train_test_split(data_l_n,data_r_n,y,test_size=0.3,random_state=813 )\n",
    "# train_l,test_l,train_r,test_r,train_y,test_y = train_test_split(data_l_n,data_r_n,y,test_size=0,random_state=5120 )\n",
    "\n",
    "ls = np.array([len(l) for l in train_l])\n",
    "ls.mean(),ls.max(),ls.min(),ls.std()\n",
    "\n",
    "malstm = get_model(input_length=max_length, embedding_length=len(word2index), \n",
    "                   n_hidden = [64,64,64],  data_dim=256, use_GPU=True, model=model,weights=None)\n",
    "\n",
    "malstm.save_weights(\"model/keras_malstm_weight.h5\")\n",
    "\n",
    "malstm.load_weights(\"model/keras_malstm_weight.h5\")\n",
    "\n",
    "malstm.save(\"model/keras_malstm.h5\")\n",
    "\n",
    "malstm = load_model(\"model/keras_malstm.h5\")\n",
    "\n",
    "history.history\n",
    "\n",
    "#train\n",
    "batch_size = 10000\n",
    "# n_epoch = 10//2\n",
    "n_epoch = 80//2\n",
    "# n_epoch = 90//2\n",
    "# n_epoch = 100//2\n",
    "# gradient_clipping_norm = 1.25\n",
    "# optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
    "optimizer=Adam(lr=0.001)\n",
    "# optimizer = SGD(1e-2)\n",
    "malstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = malstm.fit(x=[np.concatenate([train_data_l_n,train_data_r_n]), np.concatenate([train_data_r_n,train_data_l_n])], \n",
    "           y=np.concatenate([train_y,train_y]),\n",
    "           class_weight={0:1/np.mean(train_y),1:1/(1-np.mean(train_y))},\n",
    "           # validation_data=(([np.asarray(test_data_l_n), np.asarray(test_data_r_n)], test_y)),\n",
    "           batch_size=batch_size, \n",
    "           epochs=n_epoch,verbose=2)\n",
    "\n",
    "\n",
    "\n",
    "# v2 to upload dropout=0.5 layer0 with another random seeds final=150epochs\n",
    "print(f1(malstm,x_l = train_data_l_n, x_r = train_data_r_n, y=train_y))\n",
    "print(f1(malstm,x_l = test_data_l_n, x_r = test_data_r_n, y=test_y))\n",
    "\n",
    "# v2 not train embedding 80epochs\n",
    "print(f1(malstm,x_l = train_data_l_n, x_r = train_data_r_n, y=train_y))\n",
    "print(f1(malstm,x_l = test_data_l_n, x_r = test_data_r_n, y=test_y))\n",
    "\n",
    "# v2 doooo train embedding 80epochs\n",
    "print(f1(malstm,x_l = train_data_l_n, x_r = train_data_r_n, y=train_y))\n",
    "print(f1(malstm,x_l = test_data_l_n, x_r = test_data_r_n, y=test_y))\n",
    "\n",
    "python3\n",
    "inpath=\"data/input10k.csv\"\n",
    "fin = open(inpath, 'r',encoding=\"utf8\")\n",
    "linenos = []\n",
    "data_l_n, data_r_n = [], []\n",
    "for line in fin:\n",
    "    lineno, s1, s2 =line.strip().split(\"\\t\")    \n",
    "    linenos.append(lineno)\n",
    "    data_l_n.append([word2index[word] for word in list(jieba.cut(star.sub(\"1\",s1))) if word in word2index]) \n",
    "    data_r_n.append([word2index[word] for word in list(jieba.cut(star.sub(\"1\",s2))) if word in word2index])\n",
    "\n",
    "train_data_l_n = pad_sequences(data_l_n, maxlen=max_length)\n",
    "train_data_r_n = pad_sequences(data_r_n, maxlen=max_length)\n",
    "y_ = malstm.predict([np.asarray(train_data_l_n), np.asarray(train_data_r_n)],batch_size=10000)\n",
    "\n",
    "# python2\n",
    "inpath=\"../../data/input10k.csv\"\n",
    "fin = open(inpath, 'r')\n",
    "linenos = []\n",
    "data_l_n, data_r_n = [], []\n",
    "for line in fin:\n",
    "    lineno, s1, s2 =line.strip().split(\"\\t\")    \n",
    "    linenos.append(lineno)\n",
    "    data_l_n.append([word2index[word.encode(\"utf8\")] for word in list(jieba.cut(star.sub(\"1\",s1))) if word.encode(\"utf8\") in word2index]) \n",
    "    data_r_n.append([word2index[word.encode(\"utf8\")] for word in list(jieba.cut(star.sub(\"1\",s2))) if word.encode(\"utf8\") in word2index])\n",
    "\n",
    "train_data_l_n = pad_sequences(data_l_n, maxlen=max_length)\n",
    "train_data_r_n = pad_sequences(data_r_n, maxlen=max_length)\n",
    "y_ = malstm.predict([np.asarray(train_data_l_n), np.asarray(train_data_r_n)],batch_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 尝试不同的语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np \n",
    "from string import punctuation\n",
    "\n",
    "import jieba\n",
    "import gensim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, auc, roc_auc_score, f1_score\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import scipy\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "new_words = \"花呗 借呗 蚂蚁花呗 蚂蚁借呗 蚂蚁森林 小黄车 飞猪 微客 宝卡 芝麻信用 亲密付 淘票票 饿了么 摩拜 滴滴 滴滴出行\".split(\" \")\n",
    "for word in new_words:\n",
    "    jieba.add_word(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、词语级N元语法词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"data/atec_nlp_sim_train.csv\",encoding=\"utf8\",sep=\"\\t\",header=None)\n",
    "data2 = pd.read_csv(\"data/atec_nlp_sim_train_add.csv\",encoding=\"utf8\",sep=\"\\t\",header=None)\n",
    "word2index = {kv.split(\"\\t\")[0]:(kv.split(\"\\t\")[1]) for kv in open(\"model/word2index.txt\",\"r\",encoding=\"utf8\").read().split(\"\\n\")}\n",
    "char2index = {kv.split(\"\\t\")[0]:(kv.split(\"\\t\")[1]) for kv in open(\"model/char2index.txt\",\"r\",encoding=\"utf8\").read().split(\"\\n\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data1.append(data2)\n",
    "data.columns=['lineno','left','right','y']\n",
    "data.drop(\"lineno\", axis=1, inplace=True)\n",
    "cut2word = lambda s:\" \".join([word2index[word] for word in jieba.cut(s) if word in word2index])\n",
    "cut2char = lambda s:\" \".join([char2index[word] for word in s if word in char2index])\n",
    "data['word_l'] = data[\"left\"].map(cut2word)\n",
    "data['word_r'] = data[\"right\"].map(cut2word)\n",
    "data['char_l'] = data[\"left\"].map(cut2char)\n",
    "data['char_r'] = data[\"right\"].map(cut2char)\n",
    "data = data.astype({'word_l':str,'word_r':str,'y':int})\n",
    "data = data.astype({'char_l':str,'char_r':str,'y':int})\n",
    "# data = data.sample(frac=1,random_state=42)\n",
    "data.to_csv(\"data/word_lr_y.csv\",columns=['word_l','word_r','y'], index=False)\n",
    "data.to_csv(\"data/char_lr_y.csv\",columns=['char_l','char_r','y'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 27 epochs took 2 seconds\n",
      "0.817720530835285\n",
      "0.1163670766319773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.3s finished\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/word_lr_y.csv\")\n",
    "xl_train, xl_test, xr_train, xr_test, y_train, y_test = train_test_split(\n",
    "    data['word_l'],\n",
    "    data['word_r'],\n",
    "    data['y'], \n",
    "    test_size=0.1, \n",
    "    random_state=42)\n",
    "\n",
    "vectorizer_word = TfidfVectorizer(max_features=40000,\n",
    "                             min_df=5, \n",
    "                             max_df=0.5, \n",
    "                             analyzer='word',\n",
    "                             tokenizer=str.split,\n",
    "                             ngram_range=(1, 2))\n",
    "\n",
    "vectorizer_word.fit(xl_train.astype(str))\n",
    "\n",
    "tfidf_matrix_word_l_train = vectorizer_word.transform(xl_train.astype(str))\n",
    "tfidf_matrix_word_r_train = vectorizer_word.transform(xr_train.astype(str))\n",
    "tfidf_matrix_word_l_test = vectorizer_word.transform(xl_test.astype(str))\n",
    "tfidf_matrix_word_r_test = vectorizer_word.transform(xr_test.astype(str))\n",
    "xw_train = hstack([tfidf_matrix_word_l_train,tfidf_matrix_word_r_train])\n",
    "xw_test = hstack([tfidf_matrix_word_l_test,tfidf_matrix_word_r_test])\n",
    "\n",
    "lr_word = LogisticRegression(solver='sag', verbose=2)\n",
    "lr_word.fit(xw_train, y_train)\n",
    "pd.to_pickle(lr_word,\"model/lr_word_ngram.pkl\")\n",
    "y_pred_word = lr_word.predict(xw_test)\n",
    "print(accuracy_score(y_test, y_pred_word))\n",
    "print(f1_score(y_test, y_pred_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity,manhattan_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、字符级N元语法词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 24 epochs took 4 seconds\n",
      "0.8180132708821234\n",
      "0.1231781852374236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.8s finished\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/char_lr_y.csv\")\n",
    "xl_train, xl_test, xr_train, xr_test, y_train, y_test = train_test_split(\n",
    "    data['char_l'],\n",
    "    data['char_r'],\n",
    "    data['y'], \n",
    "    test_size=0.1, \n",
    "    random_state=42)\n",
    "\n",
    "vectorizer_char = TfidfVectorizer(max_features=40000,\n",
    "                             min_df=5, \n",
    "                             max_df=0.5, \n",
    "                             analyzer='word',\n",
    "                             tokenizer=str.split,\n",
    "                             ngram_range=(1, 4))\n",
    "vectorizer_char.fit(xl_train.astype(str))\n",
    "tfidf_matrix_char_l_train = vectorizer_char.transform(xl_train.astype(str))\n",
    "tfidf_matrix_char_r_train = vectorizer_char.transform(xr_train.astype(str))\n",
    "tfidf_matrix_char_l_test = vectorizer_char.transform(xl_test.astype(str))\n",
    "tfidf_matrix_char_r_test = vectorizer_char.transform(xr_test.astype(str))\n",
    "xc_train = hstack([tfidf_matrix_char_l_train,tfidf_matrix_char_r_train])\n",
    "xc_test = hstack([tfidf_matrix_char_l_test,tfidf_matrix_char_r_test])\n",
    "\n",
    "lr_char = LogisticRegression(solver='sag', verbose=2)\n",
    "lr_char.fit(xc_train, y_train)\n",
    "pd.to_pickle(lr_char,\"model/lr_char_ngram.pkl\")\n",
    "y_pred_char = lr_char.predict(xc_test)\n",
    "print(accuracy_score(y_test, y_pred_char))\n",
    "print(f1_score(y_test, y_pred_char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、词语字符混合的词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 33 epochs took 8 seconds\n",
      "0.8137197501951601\n",
      "0.17323516673884798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.3s finished\n"
     ]
    }
   ],
   "source": [
    "x_train =  hstack((xw_train, xc_train))\n",
    "x_test =  hstack((xw_test, xc_test))\n",
    "\n",
    "lr_word_char = LogisticRegression(solver='sag', verbose=2)\n",
    "lr_word_char.fit(x_train, y_train)\n",
    "\n",
    "y_pred_word_char = lr_word_char.predict(x_test)\n",
    "joblib.dump(lr_word_char, 'model/lr_word_char_ngram.pkl')\n",
    "print(accuracy_score(y_test, y_pred_word_char))\n",
    "print(f1_score(y_test, y_pred_word_char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用深度学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\servers\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Input, Dense, Embedding, Conv1D, Conv2D, MaxPooling1D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import load_model\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、不带预训练embedding的RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.360679571912149"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.rand(100)\n",
    "b = np.random.rand(100)\n",
    "np.sqrt(np.sum(np.square(a-b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02200343, 0.93163537, 0.34915694, 0.97890058, 0.41793657,\n",
       "       0.92822038, 0.66129658, 0.4074733 , 0.05131629, 0.78684462,\n",
       "       0.28771862, 0.7183421 , 0.58194868, 0.23739771, 0.20851563,\n",
       "       0.99165217, 0.86443712, 0.18590148, 0.76759527, 0.17162386,\n",
       "       0.50124604, 0.82183716, 0.95769675, 0.12618139, 0.81610265,\n",
       "       0.91092577, 0.61731606, 0.13960377, 0.78194663, 0.55549182,\n",
       "       0.24617731, 0.27751679, 0.14552345, 0.67990968, 0.52002897,\n",
       "       0.1269235 , 0.5776879 , 0.92471849, 0.70805826, 0.87439314,\n",
       "       0.93966925, 0.61134024, 0.48642969, 0.34889364, 0.59751565,\n",
       "       0.63723199, 0.25088438, 0.78221482, 0.62796302, 0.7741909 ,\n",
       "       0.82196031, 0.66530585, 0.92641323, 0.88175574, 0.4904638 ,\n",
       "       0.01458782, 0.50687491, 0.08705664, 0.58669551, 0.46216228,\n",
       "       0.62808414, 0.14209072, 0.7150203 , 0.99893692, 0.88368208,\n",
       "       0.41440839, 0.38502068, 0.06766226, 0.56548588, 0.63227811,\n",
       "       0.76386514, 0.62517842, 0.31849111, 0.09044434, 0.43898468,\n",
       "       0.96455333, 0.46444891, 0.08918807, 0.0483451 , 0.15345982,\n",
       "       0.16126589, 0.02472531, 0.67645885, 0.0777405 , 0.63974398,\n",
       "       0.43625495, 0.54180735, 0.08351283, 0.22326917, 0.28877883,\n",
       "       0.92931502, 0.93385405, 0.40997833, 0.78119876, 0.15005465,\n",
       "       0.60176901, 0.39361617, 0.38304335, 0.56166298, 0.46718117])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
